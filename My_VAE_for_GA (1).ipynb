{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import argparse\n",
    "import logging\n",
    "import boto3\n",
    "import atexit\n",
    "import torch\n",
    "import shutil\n",
    "import os, errno, time, sys\n",
    "import ipywidgets as widgets\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import torch.optim as optim\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from deap import base\n",
    "from deap import creator\n",
    "from deap import base, creator, tools, algorithms\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import interactive\n",
    "interactive(True)\n",
    "import scipy.stats as sts\n",
    "import scipy as sc\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "from datetime import timedelta\n",
    "import copy\n",
    "\n",
    "from torch.cuda import is_available\n",
    "from catboost import CatBoostClassifier, Pool, cv\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.calibration import CalibratedClassifierCV, calibration_curve\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from bayes_opt import BayesianOptimization\n",
    "from datetime import datetime, timedelta\n",
    "from trains import logger\n",
    "import operator\n",
    "import re\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "import mpld3\n",
    "from IPython import display\n",
    "import six\n",
    "import multiprocessing\n",
    "import random\n",
    "import networkx\n",
    "import time\n",
    "from IPython import display\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "mpld3.enable_notebook()\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data frame with features starts here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sorted = pd.read_csv(\"df_sorted.csv\",low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_features = ['business_group_name', 'trading_platform',\n",
    "                'country_code_iso2', 'language_name', 'channel',\n",
    "                'white_label','identifier_app_source','publisher_name', 'tag_name',\n",
    "                'affiliate_id','email_provider',\n",
    "                ]\n",
    "\n",
    "\n",
    "cyc_features = ['su_dayofweek']\n",
    "\n",
    "bin_features = ['is_allowed_bonus', 'was_demo'] \n",
    "\n",
    "num_features = ['yob',  'su_year']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_cyclic_value(x, uniques, func):\n",
    "    v = round(func(2 * np.pi * x / uniques), 6)\n",
    "    return v\n",
    "\n",
    "cyc_features_trig = []\n",
    "for c in cyc_features:\n",
    "    for f in [np.sin, np.cos]:\n",
    "        new_c = c + '_' + str(f.__name__).split('.')[-1]\n",
    "        cyc_features_trig.append(new_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapToNum(x,DICT):\n",
    "    v = DICT[x]\n",
    "    return v\n",
    "DICT = dict(zip(cat_features, {}))\n",
    "cat_features_num = []\n",
    "for c in cat_features:\n",
    "    new_c = c+\"_num\"\n",
    "    cat_features_num.append(new_c)\n",
    "    DICT[c] = dict(zip(df_sorted[c].unique(), np.arange(0,df_sorted[c].nunique())))\n",
    "    df_sorted[new_c] = df_sorted[c].apply(lambda x: mapToNum(x, DICT[c])).copy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_local_dir_if_needed(local_files_dir):\n",
    "    try:\n",
    "        os.makedirs(local_files_dir)\n",
    "    except OSError as e:\n",
    "        if e.errno != errno.EEXIST:\n",
    "            raise\n",
    "        else:\n",
    "            print(\"directory %s already exists\" % local_files_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['business_group_name', 'trading_platform', 'country_code_iso2', 'language_name', 'channel', 'white_label', 'identifier_app_source', 'publisher_name', 'tag_name', 'affiliate_id', 'email_provider', 'is_allowed_bonus', 'was_demo', 'su_dayofweek_sin', 'su_dayofweek_cos', 'yob', 'su_year']\n"
     ]
    }
   ],
   "source": [
    "all_features = cat_features + bin_features + cyc_features_trig + num_features\n",
    "all_features2 = cat_features_num + bin_features + cyc_features_trig + num_features\n",
    "cat_features_indices = list(range(len(cat_features)))\n",
    "label = 'has_first_deposit_amount'\n",
    "print(all_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sorted[cat_features_num] = df_sorted[cat_features_num].astype(int).copy()\n",
    "df_sorted[cat_features] = df_sorted[cat_features].astype(str).copy()\n",
    "df_sorted[bin_features] = df_sorted[bin_features].astype('float32').copy()\n",
    "df_sorted[num_features] = df_sorted[num_features].astype('float32').copy()\n",
    "df_sorted[label] = df_sorted[label].astype('float32').copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "business_group_name      0\n",
       "trading_platform         0\n",
       "country_code_iso2        0\n",
       "language_name            0\n",
       "channel                  0\n",
       "white_label              0\n",
       "identifier_app_source    0\n",
       "publisher_name           0\n",
       "tag_name                 0\n",
       "affiliate_id             0\n",
       "email_provider           0\n",
       "is_allowed_bonus         0\n",
       "was_demo                 0\n",
       "su_dayofweek_sin         0\n",
       "su_dayofweek_cos         0\n",
       "yob                      0\n",
       "su_year                  0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yob_avg = df_sorted.yob.mean()\n",
    "df_sorted['yob'].fillna(yob_avg,inplace=True)\n",
    "df_sorted['was_demo'].fillna(0,inplace=True)\n",
    "np.sum(df_sorted[all_features].isna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['business_group_name_num', 'trading_platform_num', 'country_code_iso2_num', 'language_name_num', 'channel_num', 'white_label_num', 'identifier_app_source_num', 'publisher_name_num', 'tag_name_num', 'affiliate_id_num', 'email_provider_num', 'is_allowed_bonus', 'was_demo', 'su_dayofweek_sin', 'su_dayofweek_cos', 'yob', 'su_year']\n"
     ]
    }
   ],
   "source": [
    "all_cols = all_features.copy()\n",
    "all_cols.append(label)\n",
    "all_cols2 = all_features2.copy()\n",
    "all_cols2.append(label)\n",
    "all_features = list(dict.fromkeys(all_features))\n",
    "all_features2 = list(dict.fromkeys(all_features2))\n",
    "all_cols = list(dict.fromkeys(all_cols))\n",
    "all_cols2 = list(dict.fromkeys(all_cols2))\n",
    "print(all_features2)\n",
    "dataset1 = df_sorted[all_cols].copy()\n",
    "dataset2 = df_sorted[all_cols2].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (686575, 17), y shape: (686575,)\n",
      "y mean is : 0.2668\n",
      "X_train shape: (683142, 17), y_train shape: (683142,)\n",
      "y_train mean is : 0.2675\n",
      "X_val shape: (3433, 17), y_val shape: (3433,)\n",
      "y_val mean is : 0.1180\n"
     ]
    }
   ],
   "source": [
    "train_size = 0.995\n",
    "\n",
    "N = df_sorted.shape[0]\n",
    "data_size = df_sorted.shape[0]\n",
    "\n",
    "permute = np.array(range(N))\n",
    "\n",
    "train_indexes = np.sort(permute[:int(train_size * N)])\n",
    "val_indexes = np.sort(permute[int((train_size) * N):])\n",
    "\n",
    "\n",
    "\n",
    "X, y = dataset1[all_features].values, dataset1[label].values\n",
    "X2, y2 = dataset2[all_features2].values, dataset2[label].values\n",
    "\n",
    "df_train = dataset1[all_features].iloc[train_indexes]\n",
    "df_test = dataset1[all_features].iloc[val_indexes]\n",
    "df_train2 = dataset2[all_features2].iloc[train_indexes]\n",
    "df_test2 = dataset2[all_features2].iloc[val_indexes]\n",
    "\n",
    "X_train, y_train = X[train_indexes, :], y[train_indexes]\n",
    "X_val, y_val = X[val_indexes, :], y[val_indexes]\n",
    "\n",
    "X2_train, y2_train = X2[train_indexes, :], y2[train_indexes]\n",
    "X2_val, y2_val = X2[val_indexes, :], y2[val_indexes]\n",
    "\n",
    "\n",
    "print(\"X shape: %s, y shape: %s\"%(X.shape, y.shape))\n",
    "print(\"y mean is : %.4f\" %np.average(y))\n",
    "print(\"X_train shape: %s, y_train shape: %s\"%(X_train.shape, y_train.shape))\n",
    "print(\"y_train mean is : %.4f\" %np.average(y_train))\n",
    "print(\"X_val shape: %s, y_val shape: %s\"%(X_val.shape, y_val.shape))\n",
    "print(\"y_val mean is : %.4f\" %np.average(y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_group_name</th>\n",
       "      <th>trading_platform</th>\n",
       "      <th>country_code_iso2</th>\n",
       "      <th>language_name</th>\n",
       "      <th>channel</th>\n",
       "      <th>white_label</th>\n",
       "      <th>identifier_app_source</th>\n",
       "      <th>publisher_name</th>\n",
       "      <th>tag_name</th>\n",
       "      <th>affiliate_id</th>\n",
       "      <th>email_provider</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>683142</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>120</td>\n",
       "      <td>15</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>215</td>\n",
       "      <td>237</td>\n",
       "      <td>242</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>683143</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>114</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>104</td>\n",
       "      <td>34</td>\n",
       "      <td>35</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>683144</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>73</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>215</td>\n",
       "      <td>237</td>\n",
       "      <td>242</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>683145</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>69</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>73</td>\n",
       "      <td>55</td>\n",
       "      <td>56</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>683146</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>147</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>178</td>\n",
       "      <td>292</td>\n",
       "      <td>272</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        business_group_name  trading_platform  country_code_iso2  \\\n",
       "683142                    3                 0                120   \n",
       "683143                    2                 0                114   \n",
       "683144                    3                 1                 73   \n",
       "683145                    3                 0                 69   \n",
       "683146                    6                 0                147   \n",
       "\n",
       "        language_name  channel  white_label  identifier_app_source  \\\n",
       "683142             15        5            2                      3   \n",
       "683143              4        0            2                      3   \n",
       "683144              4        5            2                      5   \n",
       "683145             10        0            2                      5   \n",
       "683146              4        0            2                      5   \n",
       "\n",
       "        publisher_name  tag_name  affiliate_id  email_provider  \n",
       "683142             215       237           242              31  \n",
       "683143             104        34            35              52  \n",
       "683144             215       237           242              31  \n",
       "683145              73        55            56              31  \n",
       "683146             178       292           272              31  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "label_encoders = {}\n",
    "for cat_col in cat_features:\n",
    "    label_encoders[cat_col] = LabelEncoder()\n",
    "    df_test[cat_col] = label_encoders[cat_col].fit_transform(df_test[cat_col])\n",
    "df_test[cat_features].head()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational auto encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardNN(nn.Module):\n",
    "    def __init__(self,  lin_layer_sizes,lin_layer_dropouts,output_size, actiavtions,EncOrDec):\n",
    "        \n",
    "        \"\"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        \n",
    "        lin_layer_sizes: List of integers.\n",
    "        The size of each linear layer. The length will be equal\n",
    "        to the total number\n",
    "        of linear layers in the network.\n",
    "        \n",
    "        lin_layer_dropouts: List of floats\n",
    "        The dropouts to be used after each linear layer.\n",
    "\n",
    "        output_size: Integer\n",
    "        The size of the final output.\n",
    "        \n",
    "        actiavtions: List of integers.\n",
    "        1:relu\n",
    "        2:hardtanh\n",
    "        3:elu\n",
    "        4:selu\n",
    "        5:leaky_relu\n",
    "        6:celu\n",
    "        7:rrelu\n",
    "        8:glu\n",
    "        9:gelu\n",
    "        10:logsigmoid\n",
    "        11:hardshrink\n",
    "        12:tanhshrink\n",
    "        13:softsign\n",
    "        14:softplus\n",
    "        15:softmin\n",
    "        16:softmax\n",
    "        17:softshrink\n",
    "        18:tanh\n",
    "        19:sigmoid\n",
    "        20:linear\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.lin_layers = nn.ModuleList([nn.Linear(lin_layer_sizes[i],\n",
    "                          lin_layer_sizes[i + 1]) for i in range(len(lin_layer_sizes) - 1)])\n",
    "        for lin_layer in self.lin_layers:\n",
    "            nn.init.kaiming_normal_(lin_layer.weight.data)\n",
    "        # Output Layer\n",
    "        self.output_layer = nn.Linear(lin_layer_sizes[-1],\n",
    "                                      output_size)\n",
    "        nn.init.kaiming_normal_(self.output_layer.weight.data)\n",
    "        \n",
    "        # Batch Norm Layers\n",
    "        \n",
    "        self.bn_layers = nn.ModuleList([nn.BatchNorm1d(size)\n",
    "                                            for size in lin_layer_sizes[1:]])\n",
    "\n",
    "        # Dropout Layers\n",
    "        \n",
    "        self.droput_layers = nn.ModuleList([nn.Dropout(size)#, inplace=True\n",
    "                                      for size in lin_layer_dropouts])\n",
    "        self.actiavtions = actiavtions\n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "\n",
    "        for lin_layer, dropout_layer, actiavtions in\\\n",
    "            zip(self.lin_layers, self.droput_layers, self.actiavtions):#bn_layer ,self.bn_layers,\n",
    "                if actiavtions == 1:\n",
    "                    x = F.relu(lin_layer(x))\n",
    "                if actiavtions == 2:\n",
    "                    x = F.hardtanh_(lin_layer(x))\n",
    "                if actiavtions == 3:\n",
    "                    x = F.elu_(lin_layer(x))\n",
    "                if actiavtions == 4:\n",
    "                    x = F.selu(lin_layer(x),inplace=True)\n",
    "                if actiavtions == 5:\n",
    "                    x = F.leaky_relu_(lin_layer(x))\n",
    "                if actiavtions == 6:\n",
    "                    x = F.celu(lin_layer(x),inplace=True)\n",
    "                if actiavtions == 7:\n",
    "                    x = F.rrelu_(lin_layer(x))\n",
    "                if actiavtions == 8:\n",
    "                    x = F.gelu(lin_layer(x))\n",
    "                if actiavtions == 9:\n",
    "                    x = F.logsigmoid(lin_layer(x))\n",
    "                if actiavtions == 10:\n",
    "                    x = F.hardshrink(lin_layer(x))\n",
    "                if actiavtions == 11:\n",
    "                    x = F.tanhshrink(lin_layer(x))\n",
    "                if actiavtions == 12:\n",
    "                    x = F.softshrink(lin_layer(x))\n",
    "                if actiavtions == 13:\n",
    "                    x = torch.tanh(lin_layer(x))\n",
    "                if actiavtions == 14:\n",
    "                    x = torch.sigmoid(lin_layer(x))\n",
    "                if actiavtions == 15:\n",
    "                    x = lin_layer(x)\n",
    "\n",
    "#                 x = bn_layer(x)\n",
    "                x = dropout_layer(x)\n",
    "\n",
    "        x = self.output_layer(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1250\n",
    "dataloader = DataLoader(X2[0:3750,:].astype(float), batch_size, shuffle=True, num_workers=1)\n",
    "device =  \"cpu\"#torch.device(\"cuda\") if torch.cuda.is_available() else \"cpu\"\n",
    "latent_dim = 90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Normal(object):\n",
    "    def __init__(self, mu, sigma, log_sigma, v=None, r=None):\n",
    "        self.mu = mu.to(device)\n",
    "        self.sigma = sigma.to(device)  # either stdev diagonal itself, or stdev diagonal from decomposition\n",
    "        self.logsigma = log_sigma.to(device)\n",
    "        dim = mu.get_shape()\n",
    "        if v is None:\n",
    "            v = torch.FloatTensor(*dim)\n",
    "        if r is None:\n",
    "            r = torch.FloatTensor(*dim)\n",
    "        self.v = v.to(device)\n",
    "        self.r = r.to(device)\n",
    "        \n",
    "\n",
    "class VAE(torch.nn.Module):\n",
    "    latent_dim = latent_dim\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(VAE, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self._enc_mu = torch.nn.Linear(latent_dim,latent_dim).to(device)\n",
    "        self._enc_log_sigma = torch.nn.Linear(latent_dim,latent_dim).to(device)\n",
    "\n",
    "        \n",
    "    def _sample_latent(self, h_enc):\n",
    "        \"\"\"\n",
    "        Return the latent normal sample z ~ N(mu, sigma^2)\n",
    "        \"\"\"\n",
    "        mu = self._enc_mu(h_enc)\n",
    "        log_sigma = self._enc_log_sigma(h_enc)\n",
    "        sigma = torch.exp(log_sigma)\n",
    "        std_z = torch.from_numpy(np.random.normal(0, 1, size=sigma.size())).float()\n",
    "        self.z_mean = mu.to(device)\n",
    "        self.z_sigma = sigma.to(device)\n",
    "        std_z = std_z.to(device)\n",
    "        \n",
    "        \n",
    "        return mu.to(device) + sigma.to(device) * Variable(std_z, requires_grad=False).to(device)  # Reparameterization trick\n",
    "\n",
    "    def forward(self, x):\n",
    "        h_enc = self.encoder(x).to(device)\n",
    "        z = self._sample_latent(h_enc).to(device)\n",
    "        return self.decoder(z).to(device)\n",
    "    \n",
    "\n",
    "def latent_loss(z_mean, z_stddev):\n",
    "    mean_sq = z_mean * z_mean\n",
    "    mean_sq = mean_sq.to(device)\n",
    "    stddev_sq = z_stddev * z_stddev\n",
    "    stddev_sq = stddev_sq.to(device)\n",
    "    return 0.5 * torch.mean(mean_sq.to(device) + stddev_sq.to(device) - torch.log(stddev_sq).to(device) - 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Genearations functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = len(all_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_model(lr,\n",
    "              lin_layer_sizes_enc,lin_layer_dropouts_enc,output_size_enc, actiavtions_enc,\n",
    "              lin_layer_sizes_dec,lin_layer_dropouts_dec,output_size_dec, actiavtions_dec,\n",
    "             itter_num,DoPlot):\n",
    "    BestOfBest = dict()\n",
    "    N_neurons_enc = []\n",
    "    N_neurons_dec = []\n",
    "    f_actiavtions_enc = []\n",
    "    f_actiavtions_dec = []\n",
    "    \n",
    "\n",
    "    \n",
    "    for N_neurons in lin_layer_sizes_enc:\n",
    "        N_neurons_enc.append(int(N_neurons))\n",
    "        \n",
    "    for N_neurons in lin_layer_sizes_dec:\n",
    "        N_neurons_dec.append(int(N_neurons))\n",
    "        \n",
    "    for f in actiavtions_enc:\n",
    "        f_actiavtions_enc.append(int(f))\n",
    "        \n",
    "    for f in actiavtions_dec:\n",
    "        f_actiavtions_dec.append(int(f))    \n",
    "        \n",
    "    BestLoss = np.inf\n",
    "    input_dim = len(all_features)-1\n",
    "    \n",
    "#     device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "    transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    encoder = FeedForwardNN(N_neurons_enc , lin_layer_dropouts_enc, latent_dim,f_actiavtions_enc,\"end\")\n",
    "    decoder = FeedForwardNN(N_neurons_dec , lin_layer_dropouts_dec, input_size,f_actiavtions_dec,\"dec\")\n",
    "    encoder = encoder.to(device)\n",
    "    decoder = decoder.to(device)\n",
    "    vae = VAE(encoder, decoder)\n",
    "\n",
    "    \n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    optimizer = optim.Adam(vae.parameters(), lr=lr, betas=(0.9, 0.999), eps=1e-03,\n",
    "                             weight_decay=0.01, amsgrad=True)\n",
    "\n",
    "    l = 0\n",
    "\n",
    "    for epoch in range(itter_num):\n",
    "        start_time = time.time()\n",
    "\n",
    "        \n",
    "        for inputs in dataloader:\n",
    "            \n",
    "            \n",
    "            inputs = Variable(torch.FloatTensor(inputs.float()))\n",
    "            inputs = inputs.to(device)\n",
    "\n",
    "            \n",
    "            dec = vae(inputs)\n",
    "            dec = dec.to(device)\n",
    "\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            ll1 = latent_loss(vae.z_mean, vae.z_sigma)\n",
    "            ll2 = criterion(dec,inputs)\n",
    "            loss = ll1 + ll2\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            l = loss.data.item()\n",
    "            \n",
    "              \n",
    "        end_time = time.time()\n",
    "        epoch_time = end_time-start_time\n",
    "                            \n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "        if ll2.detach().cpu().numpy()<BestLoss:\n",
    "            BestNet = copy.copy(vae)\n",
    "            BestLoss = ll2.detach().cpu().numpy()\n",
    "        if np.isnan(ll2.detach().cpu().numpy()):\n",
    "            BestNet = []\n",
    "            BestLoss = np.inf\n",
    "            break\n",
    "        if DoPlot and epoch%10==0:\n",
    "            if epoch==0:\n",
    "                f = go.FigureWidget()\n",
    "            f.add_trace(go.Scatter(x=[epoch],y=[BestLoss],line=dict(color='firebrick', width=4,\n",
    "                                      dash='dash'),name=None))\n",
    "            f.update_layout(showlegend=False)\n",
    "            display.clear_output(wait=True)\n",
    "            display.display(f)\n",
    "            time.sleep(1)\n",
    "            f.update_layout( yaxis=dict(\n",
    "                                type=\"log\"\n",
    "                                ))\n",
    "    return BestLoss,BestNet\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#limits of neurons per layer\n",
    "layer_sizes_MIN,layer_sizes_MAX = 0,100\n",
    "#limits of dropouts \n",
    "layer_dropouts_MIN,layer_dropouts_MAX = 0,0.3\n",
    "#limits of activation functions\n",
    "actiavtions_MIN,actiavtions_MAX = 1,15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Build_model(layer_sizes_enc,layer_dropouts_enc,f_actiavtions_enc ,\n",
    "                layer_sizes_dec,layer_dropouts_dec,f_actiavtions_dec,itter_num,DoPlot):\n",
    "    \n",
    "    \n",
    "    input_dim = 17\n",
    "    output_size_enc = latent_dim\n",
    "    lr = 0.016\n",
    "    \n",
    "    \n",
    "    # encoder :\n",
    "    \n",
    "    lin_layer_sizes_enc = []\n",
    "    lin_layer_dropouts_enc = []\n",
    "    actiavtions_enc=[]\n",
    "    \n",
    "    lin_layer_sizes_enc.append(input_dim) \n",
    "    lin_layer_dropouts_enc.append(0)\n",
    "    actiavtions_enc.append(15)\n",
    "    \n",
    "    for i,l in enumerate(layer_sizes_enc):\n",
    "        if l>0:\n",
    "            lin_layer_sizes_enc.append(np.max([np.min([int(layer_sizes_enc[i]),layer_sizes_MAX]),layer_sizes_MIN]))\n",
    "\n",
    "            lin_layer_dropouts_enc.append(np.max([np.min([layer_dropouts_enc[i],layer_dropouts_MAX]),\n",
    "                                                  layer_dropouts_MIN]))\n",
    "\n",
    "            actiavtions_enc.append(np.max([np.min([int(f_actiavtions_enc[i]),actiavtions_MAX]),actiavtions_MIN]))\n",
    "        \n",
    "    # decoder :\n",
    "    \n",
    "    lin_layer_sizes_dec = []\n",
    "    lin_layer_dropouts_dec = []\n",
    "    actiavtions_dec=[]\n",
    "    \n",
    "    lin_layer_sizes_dec.append(latent_dim)\n",
    "    lin_layer_dropouts_dec.append(0)\n",
    "    actiavtions_dec.append(15)\n",
    "    \n",
    "    for i,l in enumerate(layer_sizes_enc):\n",
    "        if l>0:\n",
    "            lin_layer_sizes_dec.append(np.max([np.min([int(layer_sizes_dec[i]),layer_sizes_MAX]),layer_sizes_MIN]))\n",
    "\n",
    "            lin_layer_dropouts_dec.append(np.max([np.min([layer_dropouts_dec[i],layer_dropouts_MAX]),\n",
    "                                                 layer_dropouts_MIN]))\n",
    "\n",
    "            actiavtions_dec.append(np.max([np.min([int(f_actiavtions_dec[i]),actiavtions_MAX]),actiavtions_MIN]))\n",
    "           \n",
    "     \n",
    "    \n",
    "    output_size_dec = input_dim\n",
    "    \n",
    "    \n",
    "    layer_sizes_enc = []\n",
    "    layer_dropouts_enc = []\n",
    "    f_actiavtions_enc = []\n",
    "    for i,l in enumerate (lin_layer_sizes_enc):\n",
    "        if l>0:\n",
    "            layer_sizes_enc.append(l)\n",
    "            layer_dropouts_enc.append(lin_layer_dropouts_enc[i])\n",
    "            f_actiavtions_enc.append(actiavtions_enc[i])\n",
    "\n",
    "    layer_sizes_dec = []\n",
    "    layer_dropouts_dec = []\n",
    "    f_actiavtions_dec = []\n",
    "\n",
    "    for i,l in enumerate (lin_layer_sizes_dec):\n",
    "        if l>0:\n",
    "            layer_sizes_dec.append(l)\n",
    "            layer_dropouts_dec.append(lin_layer_dropouts_dec[i])\n",
    "            f_actiavtions_dec.append(actiavtions_dec[i])        \n",
    "    \n",
    "    \n",
    "    loss,BestModel = run_model(lr,\n",
    "                  layer_sizes_enc,layer_dropouts_enc,output_size_enc, f_actiavtions_enc,\n",
    "                  layer_sizes_dec,layer_dropouts_dec,output_size_dec, f_actiavtions_dec,itter_num,DoPlot)\n",
    "    \n",
    "    if loss == np.nan or loss == np.inf or loss == -np.inf:\n",
    "        runagain = True\n",
    "        while runagain:\n",
    "            print(loss,lr)\n",
    "            lr = lr/10\n",
    "            loss,BestModel = run_model(lr,\n",
    "                  layer_sizes_enc,layer_dropouts_enc,output_size_enc, f_actiavtions_enc,\n",
    "                  layer_sizes_dec,layer_dropouts_dec,output_size_dec, f_actiavtions_dec,itter_num,DoPlot)\n",
    "            \n",
    "            if loss == np.nan or loss == np.inf or loss == -np.inf:\n",
    "                runagain = True\n",
    "                if lr<1e-12:\n",
    "                    runagain = False                   \n",
    "            else:\n",
    "                runagain = False\n",
    "        \n",
    "    return loss,BestModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_pop_to_model(individual):\n",
    "\n",
    "    layer_sizes_enc = individual[0::6]\n",
    "    layer_dropouts_enc = individual[1::6]\n",
    "    f_actiavtions_enc = individual[2::6]\n",
    "    layer_sizes_dec = individual[3::6]\n",
    "    layer_dropouts_dec = individual[4::6]\n",
    "    f_actiavtions_dec = individual[5::6]\n",
    "    \n",
    "    loss,BestModel = Build_model(layer_sizes_enc,layer_dropouts_enc,f_actiavtions_enc ,\n",
    "                    layer_sizes_dec,layer_dropouts_dec,f_actiavtions_dec,30,False)\n",
    "    \n",
    "    return loss,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MateWithBonds(parent1,parent2):\n",
    "    \n",
    "    child = tools.cxTwoPoint(parent1,parent2)\n",
    "    layer_sizes_enc = child[0][0::6]\n",
    "    layer_dropouts_enc = child[0][1::6]\n",
    "    f_actiavtions_enc = child[0][2::6]\n",
    "    layer_sizes_dec = child[0][3::6]\n",
    "    layer_dropouts_dec = child[0][4::6]\n",
    "    f_actiavtions_dec = child[0][5::6]\n",
    "    child = []\n",
    "    for i,sizes_enc in enumerate(layer_sizes_enc):\n",
    "        sizes_enc = int(sizes_enc)\n",
    "        dropouts_enc = layer_dropouts_enc[i]\n",
    "        actiavtions_enc = int(f_actiavtions_enc[i])\n",
    "        sizes_dec = int(layer_sizes_dec[i])\n",
    "        dropouts_dec = layer_dropouts_dec[i]\n",
    "        actiavtions_dec = int(f_actiavtions_dec[i])\n",
    "        \n",
    "        \n",
    "        sizes_enc = np.min([np.max([sizes_enc,layer_sizes_MIN]),layer_sizes_MAX])\n",
    "        dropouts_enc = np.min([np.max([dropouts_enc,layer_dropouts_MIN]),layer_dropouts_MAX])\n",
    "        actiavtions_enc = np.min([np.max([actiavtions_enc,actiavtions_MIN]),actiavtions_MAX])\n",
    "        sizes_dec = np.min([np.max([sizes_dec,layer_sizes_MIN]),layer_sizes_MAX])\n",
    "        dropouts_dec = np.min([np.max([dropouts_dec,layer_dropouts_MIN]),layer_dropouts_MAX])\n",
    "        actiavtions_dec = np.min([np.max([actiavtions_dec,actiavtions_MIN]),actiavtions_MAX])\n",
    "        \n",
    "        child.append(sizes_enc)\n",
    "        child.append(dropouts_enc)\n",
    "        child.append(actiavtions_enc)\n",
    "        child.append(sizes_dec)\n",
    "        child.append(dropouts_dec)\n",
    "        child.append(actiavtions_dec)\n",
    "    child = creator.Individual(child)\n",
    "    return(parent1,child)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fillpop(df,population):\n",
    "    n = np.min([df.shape[0],len(population)])\n",
    "    for i in range(n):\n",
    "        tmp = df.individuals.iloc[i].replace('[','')\n",
    "        tmp = tmp.replace(']','')\n",
    "        tmp = tmp.split(',')\n",
    "        tmp2 = []\n",
    "        for l in tmp:\n",
    "            tmp2.append(float(l))\n",
    "        population[i] = creator.Individual(tmp2)\n",
    "        population[i].fitness.values = df.fitness.iloc[i],\n",
    "    \n",
    "    return(population)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model_after_ga(individual):\n",
    "\n",
    "    layer_sizes_enc = individual[0::6]\n",
    "    layer_dropouts_enc = individual[1::6]\n",
    "    f_actiavtions_enc = individual[2::6]\n",
    "    layer_sizes_dec = individual[3::6]\n",
    "    layer_dropouts_dec = individual[4::6]\n",
    "    f_actiavtions_dec = individual[5::6]\n",
    "    \n",
    "    loss,BestModel = Build_model(layer_sizes_enc,layer_dropouts_enc,f_actiavtions_enc ,\n",
    "                    layer_sizes_dec,layer_dropouts_dec,f_actiavtions_dec,3000,True)\n",
    "    \n",
    "    return loss,BestModel "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Genetic algorithm starts here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "creator.create(\"FitnessMin\", base.Fitness, weights=(-1.0,))\n",
    "creator.create(\"Individual\", list, fitness=creator.FitnessMin)\n",
    "\n",
    "population_size = 30\n",
    "num_generations = 100\n",
    "max_layers = 5\n",
    "gene_length = 1\n",
    "toolbox = base.Toolbox()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "    toolbox.register(\"attr_layer_sizes_enc\", random.randint, layer_sizes_MIN,layer_sizes_MAX)\n",
    "    toolbox.register(\"attr_layer_dropouts_enc\", random.uniform, layer_dropouts_MIN,layer_dropouts_MAX)\n",
    "    toolbox.register(\"attr_actiavtions_enc\", random.randint,actiavtions_MIN,actiavtions_MAX )\n",
    "    toolbox.register(\"attr_layer_sizes_dec\", random.randint, layer_sizes_MIN,layer_sizes_MAX)\n",
    "    toolbox.register(\"attr_layer_dropouts_dec\", random.uniform,layer_dropouts_MIN,layer_dropouts_MAX)\n",
    "    toolbox.register(\"attr_actiavtions_dec\", random.randint, actiavtions_MIN,actiavtions_MAX)\n",
    "    toolbox.register(\"individual\", tools.initCycle, creator.Individual,\n",
    "    (toolbox.attr_layer_sizes_enc, toolbox.attr_layer_dropouts_enc,toolbox.attr_actiavtions_enc,\n",
    "    toolbox.attr_layer_sizes_dec,toolbox.attr_layer_dropouts_dec,toolbox.attr_actiavtions_dec), n=max_layers)\n",
    "    toolbox.register('population', tools.initRepeat, list, toolbox.individual)\n",
    "    toolbox.register(\"MateWithBonds\",MateWithBonds)\n",
    "    toolbox.register(\"mate\", toolbox.MateWithBonds)\n",
    "    toolbox.register('mutate', tools.mutGaussian, mu=0, sigma=1, indpb=0.2)\n",
    "    toolbox.register('select', tools.selTournament,tournsize=5)\n",
    "    toolbox.register('evaluate', send_pop_to_model)\n",
    "\n",
    "    population = toolbox.population(n = population_size)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "hof = tools.HallOfFame(3)\n",
    "stats = tools.Statistics(lambda ind: ind.fitness.values)\n",
    "stats.register(\"avg\", np.average)\n",
    "stats.register(\"std\", np.std)\n",
    "stats.register(\"min\", min)\n",
    "stats.register(\"max\", max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# first run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "directory /home/octopus/projects/sandbox/GA_VAE/CSVs_4/ already exists\n",
      "inf 0.016\n",
      "inf 0.016\n",
      "inf 0.0016\n",
      "inf 0.00016\n",
      "inf 1.6000000000000003e-05\n",
      "inf 1.6000000000000004e-06\n",
      "inf 1.6000000000000003e-07\n",
      "inf 1.6000000000000004e-08\n",
      "inf 1.6000000000000005e-09\n",
      "inf 1.6000000000000004e-10\n",
      "inf 1.6000000000000003e-11\n",
      "inf 1.6000000000000003e-12\n",
      "inf 0.016\n",
      "inf 0.0016\n",
      "inf 0.00016\n",
      "inf 1.6000000000000003e-05\n",
      "inf 1.6000000000000004e-06\n",
      "inf 1.6000000000000003e-07\n",
      "inf 1.6000000000000004e-08\n",
      "inf 1.6000000000000005e-09\n",
      "inf 1.6000000000000004e-10\n",
      "inf 1.6000000000000003e-11\n",
      "inf 1.6000000000000003e-12\n",
      "inf 0.016\n",
      "inf 0.0016\n",
      "inf 0.00016\n",
      "inf 1.6000000000000003e-05\n",
      "inf 1.6000000000000004e-06\n",
      "inf 1.6000000000000003e-07\n",
      "inf 1.6000000000000004e-08\n",
      "inf 1.6000000000000005e-09\n",
      "inf 1.6000000000000004e-10\n",
      "inf 1.6000000000000003e-11\n",
      "inf 1.6000000000000003e-12\n",
      "inf 0.016\n",
      "inf 0.0016\n",
      "inf 0.00016\n",
      "inf 1.6000000000000003e-05\n",
      "inf 1.6000000000000004e-06\n",
      "inf 1.6000000000000003e-07\n",
      "inf 1.6000000000000004e-08\n",
      "inf 1.6000000000000005e-09\n",
      "inf 1.6000000000000004e-10\n",
      "inf 1.6000000000000003e-11\n",
      "inf 1.6000000000000003e-12\n",
      "inf 0.016\n",
      "inf 0.0016\n",
      "inf 0.00016\n",
      "inf 1.6000000000000003e-05\n",
      "inf 1.6000000000000004e-06\n",
      "inf 1.6000000000000003e-07\n",
      "inf 1.6000000000000004e-08\n",
      "inf 1.6000000000000005e-09\n",
      "inf 1.6000000000000004e-10\n",
      "inf 1.6000000000000003e-11\n",
      "inf 1.6000000000000003e-12\n",
      "inf 0.016\n",
      "inf 0.0016\n",
      "inf 0.00016\n",
      "inf 1.6000000000000003e-05\n",
      "inf 1.6000000000000004e-06\n",
      "inf 1.6000000000000003e-07\n",
      "inf 1.6000000000000004e-08\n",
      "inf 1.6000000000000005e-09\n",
      "inf 1.6000000000000004e-10\n",
      "inf 1.6000000000000003e-11\n",
      "inf 1.6000000000000003e-12\n",
      "inf 0.016\n",
      "inf 0.0016\n",
      "inf 0.00016\n",
      "inf 1.6000000000000003e-05\n",
      "inf 1.6000000000000004e-06\n",
      "inf 1.6000000000000003e-07\n",
      "inf 1.6000000000000004e-08\n",
      "inf 1.6000000000000005e-09\n",
      "inf 1.6000000000000004e-10\n",
      "inf 1.6000000000000003e-11\n",
      "inf 1.6000000000000003e-12\n",
      "gen\tnevals\tavg\tstd\tmin                 \tmax   \n",
      "0  \t30    \tinf\tnan\t(3656.250244140625,)\t(inf,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/octopus/virtualenv/lib/python3.6/site-packages/numpy/core/_methods.py:117: RuntimeWarning:\n",
      "\n",
      "invalid value encountered in subtract\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inf 0.016\n",
      "1  \t15    \t66500.5\t139050\t(2563.775146484375,)\t(471036.34375,)\n"
     ]
    }
   ],
   "source": [
    "CSVsPath = '/home/octopus/projects/sandbox/GA_VAE/CSVs_4/'\n",
    "\n",
    "create_local_dir_if_needed(CSVsPath)\n",
    "\n",
    "r = algorithms.eaSimple(population, toolbox, cxpb = 0.33, mutpb = 0.25, ngen = 1,stats=stats,\n",
    "                                 halloffame = hof,verbose = True)\n",
    "\n",
    "filename = \"gen_0.csv\"\n",
    "fitness = []\n",
    "for i,pop in enumerate(population):\n",
    "    fitness.append(population[i].fitness.values[0])\n",
    "df = pd.DataFrame({\"individuals\":population,\"fitness\":fitness})\n",
    "df.to_csv(CSVsPath + filename)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/octopus/virtualenv/lib/python3.6/site-packages/numpy/core/_methods.py:117: RuntimeWarning:\n",
      "\n",
      "invalid value encountered in subtract\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>avg</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>generation</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>inf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1084.560059</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            avg  std          min  max\n",
       "generation                            \n",
       "7           inf  NaN  1084.560059  inf"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CSVsPath = '/home/octopus/projects/sandbox/GA_VAE/CSVs_4/'\n",
    "gen = 7\n",
    "filename = \"gen_\"+str(gen)+\".csv\"\n",
    "df = pd.read_csv(CSVsPath + filename)\n",
    "population = fillpop(df,population)\n",
    "Q = stats.compile(population)\n",
    "stats_df = pd.DataFrame(Q)\n",
    "stats_df['generation'] = gen\n",
    "stats_df.set_index('generation',inplace=True)\n",
    "stats_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe66c6d984fb4ca094c89f09aa4ae2ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureWidget({\n",
       "    'data': [{'line': {'color': 'firebrick', 'dash': 'dash', 'width': 4},\n",
       "              'mode':â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gen\tnevals\tavg    \tstd    \tmin                 \tmax                 \n",
      "0  \t0     \t1017.46\t567.646\t(809.1901245117188,)\t(3185.023193359375,)\n",
      "1  \t17    \t23222.9\t85564.3\t(809.1901245117188,)\t(405921.0625,)      \n"
     ]
    }
   ],
   "source": [
    "\n",
    "f = go.FigureWidget()\n",
    "\n",
    "\n",
    "for gen in range(gen,num_generations):\n",
    "    Q = stats.compile(population)\n",
    "    f.add_trace(go.Scatter(x=[gen],y=[Q['min'][0]], mode='markers',line=dict(color='firebrick', width=4,\n",
    "                          dash='dash'),name=None))\n",
    "    f.update_layout(showlegend=False)\n",
    "    display.clear_output(wait=True)\n",
    "    display.display(f)\n",
    "    time.sleep(1)\n",
    "    f.update_layout( yaxis=dict(\n",
    "                        type=\"log\"\n",
    "                        ))\n",
    "    filename = \"gen_\"+str(gen)+\".csv\"\n",
    "    df = pd.read_csv(CSVsPath + filename)\n",
    "    population = fillpop(df,population)\n",
    "    r = algorithms.eaSimple(population, toolbox, cxpb = 0.33, mutpb = 0.25, ngen = 1,stats=stats,\n",
    "                                 halloffame = hof,verbose = True)\n",
    "    fitness = []\n",
    "    for i,pop in enumerate(population):\n",
    "        fitness.append(population[i].fitness.values[0])\n",
    "    df = pd.DataFrame({\"individuals\":population,\"fitness\":fitness})\n",
    "    filename = \"gen_\"+str(gen+1)+\".csv\"\n",
    "    df.to_csv(CSVsPath + filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.read_csv(filename)\n",
    "# df.fitness = df.fitness.apply(lambda x: float(x.replace('(','').replace(')','').replace(',','')))\n",
    "df.sort_values(by='fitness',inplace=True,ascending=False)\n",
    "\n",
    "df.drop_duplicates(subset=['fitness'],inplace=True)\n",
    "# df.reset_index(inplace=True)\n",
    "population = fillpop(df,population)\n",
    "BestModel = population[0]\n",
    "\n",
    "df.to_csv(\"gen.csv\")\n",
    "df.head(300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss,BestModel = run_model_after_ga(BestModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "fitness = []\n",
    "for pop in population:\n",
    "    fitness.append(pop.fitness.wvalues[0])\n",
    "    \n",
    "cp = dict(population=population,fitness=fitness, generation=1, halloffame=hof,\n",
    "                       rndstate=random.getstate())\n",
    "\n",
    "with open(\"checkpoint_name.pkl\", \"wb\") as cp_file:\n",
    "    pickle.dump(cp, cp_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"checkpoint_name.pkl\", \"r\") as cp_file:\n",
    "    cp = pickle.load(\"checkpoint_name.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp['fitness']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pop in population:\n",
    "    print(vars(stats))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = dataloader.dataset[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = Variable(torch.FloatTensor(inputs))\n",
    "inputs = inputs.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = BestModel(inputs.reshape(1,17))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_cat = outputs[0]\n",
    "out_cat.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "true = []\n",
    "pred = []\n",
    "for i,cat in enumerate(cat_features):\n",
    "    m = np.max(list(DICT[cat].values()))\n",
    "    true.append(list(DICT[cat].keys())[list(DICT[cat].values()).index(int(inputs[i]))])\n",
    "    pred.append(list(DICT[cat].keys())[list(DICT[cat].values()).index(int(np.min([out_cat[i],m])))])\n",
    "    print(list(DICT[cat].keys())[list(DICT[cat].values()).index(int(inputs[i]))],\"\\t : \\t\",\n",
    "        list(DICT[cat].keys())[list(DICT[cat].values()).index(int(np.min([out_cat[i],m])))])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,f in enumerate(all_features[len(cat_features):]):\n",
    "    m = len(cat_features)+i\n",
    "    true.append(inputs[m].detach().cpu().numpy())\n",
    "    pred.append(out_cat[m].detach().cpu().numpy())\n",
    "    print(f,\":\\ninput : \",inputs[m].detach().cpu().numpy(),\" - output : \",out_cat[m].detach().cpu().numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_res = pd.DataFrame({\"feature\":all_features,\"input\":true,\"output\":pred})\n",
    "df_res.head(17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lat_vec = []\n",
    "for x in dataloader:\n",
    "    for y in x:\n",
    "        inputs = Variable(torch.FloatTensor(y.float()))\n",
    "        inputs = inputs.to(device)\n",
    "        l_vec = BestNet.encoder(inputs)\n",
    "        lat_vec.append(l_vec.detach().cpu().numpy())\n",
    "lat_vec    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(lat_vec)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx = []\n",
    "yy = []\n",
    "for x,y in pca.transform(lat_vec):\n",
    "    xx.append(x)\n",
    "    yy.append(y)\n",
    "    \n",
    "_=plt.scatter(xx,yy)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
